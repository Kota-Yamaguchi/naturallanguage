{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.0-cp38-cp38-manylinux2010_x86_64.whl (394.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.8 MB 30 kB/s  eta 0:00:01   |▍                               | 5.1 MB 1.1 MB/s eta 0:05:51     |█▏                              | 13.9 MB 1.5 MB/s eta 0:04:09     |█▍                              | 16.8 MB 2.0 MB/s eta 0:03:09     |█▊                              | 21.7 MB 1.8 MB/s eta 0:03:30     |██▍                             | 29.3 MB 1.4 MB/s eta 0:04:19     |██▍                             | 30.1 MB 1.6 MB/s eta 0:03:53     |███▋                            | 43.9 MB 1.4 MB/s eta 0:04:20     |████                            | 49.3 MB 1.2 MB/s eta 0:04:45     |████▏                           | 50.7 MB 1.1 MB/s eta 0:05:21     |████▎                           | 53.4 MB 1.9 MB/s eta 0:02:56     |████▍                           | 53.9 MB 1.9 MB/s eta 0:02:56     |████▌                           | 55.4 MB 1.5 MB/s eta 0:03:48     |████▌                           | 55.5 MB 1.5 MB/s eta 0:03:48     |████▋                           | 57.4 MB 2.9 MB/s eta 0:01:56     |█████                           | 62.1 MB 2.8 MB/s eta 0:01:57     |█████▌                          | 67.3 MB 1.4 MB/s eta 0:03:50     |██████▉                         | 84.7 MB 4.0 MB/s eta 0:01:19     |███████▍                        | 90.6 MB 2.3 MB/s eta 0:02:10     |███████▍                        | 91.2 MB 2.3 MB/s eta 0:02:10     |███████▋                        | 93.3 MB 1.9 MB/s eta 0:02:36     |████████▋                       | 106.0 MB 1.6 MB/s eta 0:03:04     |████████▊                       | 107.2 MB 1.7 MB/s eta 0:02:49     |█████████▏                      | 113.2 MB 2.3 MB/s eta 0:02:03     |█████████▊                      | 119.9 MB 1.3 MB/s eta 0:03:36     |██████████                      | 123.1 MB 610 kB/s eta 0:07:26     |██████████▎                     | 126.3 MB 1.3 MB/s eta 0:03:32     |██████████▍                     | 128.3 MB 749 kB/s eta 0:05:56     |██████████▋                     | 130.9 MB 1.1 MB/s eta 0:04:02     |██████████▉                     | 133.3 MB 840 kB/s eta 0:05:12     |███████████▎                    | 139.6 MB 1.1 MB/s eta 0:03:43     |███████████▌                    | 141.6 MB 1.8 MB/s eta 0:02:22     |████████████                    | 149.1 MB 825 kB/s eta 0:04:58     |████████████▌                   | 154.2 MB 1.4 MB/s eta 0:02:47     |████████████▋                   | 155.8 MB 1.1 MB/s eta 0:03:37     |█████████████                   | 159.4 MB 1.0 MB/s eta 0:03:55     |█████████████▍                  | 164.6 MB 619 kB/s eta 0:06:12     |█████████████▋                  | 167.7 MB 1.4 MB/s eta 0:02:43     |█████████████▉                  | 170.3 MB 1.0 MB/s eta 0:03:43     |██████████████▎                 | 175.8 MB 2.0 MB/s eta 0:01:52     |██████████████▍                 | 177.7 MB 1.3 MB/s eta 0:02:45     |███████████████▏                | 187.6 MB 581 kB/s eta 0:05:57     |███████████████▌                | 191.7 MB 926 kB/s eta 0:03:40     |████████████████▏               | 199.8 MB 1.0 MB/s eta 0:03:11     |████████████████▍               | 201.7 MB 280 kB/s eta 0:11:30     |████████████████▋               | 204.3 MB 1.1 MB/s eta 0:02:55     |████████████████▊               | 206.8 MB 1.7 MB/s eta 0:01:49     |██████████████████              | 221.0 MB 2.4 MB/s eta 0:01:12     |██████████████████              | 222.6 MB 3.9 MB/s eta 0:00:45     |████████████████████            | 246.2 MB 768 kB/s eta 0:03:14     |████████████████████            | 246.9 MB 768 kB/s eta 0:03:13     |████████████████████▏           | 249.3 MB 1.2 MB/s eta 0:01:59     |█████████████████████▌          | 265.8 MB 1.1 MB/s eta 0:01:54     |███████████████████████▍        | 287.9 MB 1.5 MB/s eta 0:01:14     |███████████████████████▋        | 290.6 MB 1.7 MB/s eta 0:01:02     |███████████████████████▊        | 292.2 MB 1.7 MB/s eta 0:01:01     |███████████████████████▊        | 292.8 MB 2.3 MB/s eta 0:00:44     |████████████████████████▎       | 300.1 MB 557 kB/s eta 0:02:50     |█████████████████████████▎      | 312.4 MB 1.3 MB/s eta 0:01:06     |█████████████████████████▍      | 312.8 MB 1.3 MB/s eta 0:01:06     |██████████████████████████▍     | 325.7 MB 1.3 MB/s eta 0:00:52     |███████████████████████████▊    | 342.2 MB 1.2 MB/s eta 0:00:45     |███████████████████████████▉    | 344.0 MB 1.2 MB/s eta 0:00:44     |████████████████████████████▍   | 350.9 MB 2.8 MB/s eta 0:00:16     |████████████████████████████▌   | 351.0 MB 2.8 MB/s eta 0:00:16     |██████████████████████████████▉ | 381.0 MB 1.2 MB/s eta 0:00:12     |███████████████████████████████ | 381.8 MB 1.7 MB/s eta 0:00:08\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 1.0 MB/s eta 0:00:01    |██████████                      | 3.3 MB 1.4 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 927 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 817 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 585 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.13.0)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 613 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.35.1)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.19.4)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 739 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 462 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 397 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (49.6.0.post20201009)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 708 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 658 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 585 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.0.1)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 798 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=81744 sha256=6129ff5921499e66060dc33fc9d9e635611fae8a99ca1148018ec42ed6b0f195\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=68fc37d6d0a61f26b490ec504727084a96c7b06c7a8684c091108cf85c2d6569\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built wrapt termcolor\n",
      "Installing collected packages: markdown, grpcio, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard-plugin-wit, werkzeug, absl-py, tensorboard, keras-preprocessing, tensorflow-estimator, opt-einsum, gast, astunparse, google-pasta, flatbuffers, wrapt, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.3 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f87e2f84c27b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"You say goodbye and I say hello.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/common/util.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mnew_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mid_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from common.util import preprocess\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "corpus[1:-1]\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    target=corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs =[]\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t==0:\n",
    "                continue\n",
    "            cs.append(corpus[idx+t])\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus)\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''one-hot表現への変換\n",
    "    :param corpus: 単語IDのリスト（1次元もしくは2次元のNumPy配列）\n",
    "    :param vocab_size: 語彙数\n",
    "    :return: one-hot表現（2次元もしくは3次元のNumPy配列）\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.float32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.float32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "print(contexts)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size=7, hidden_size=2, epochs = 10000, optimizer='sgd'):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        weight = tf.constant(0.01,dtype='float64')\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.W_in = tf.Variable(tf.random.normal([V, H]), dtype=\"float32\")\n",
    "        #self.b1 = tf.Variable(tf.random.normal([self.embedding_dim])) #bias\n",
    " \n",
    "        self.W_out = tf.Variable(tf.random.normal([H, V]), dtype=\"float32\")\n",
    "        #self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n",
    "        \n",
    "        if optimizer=='adam':\n",
    "            self.optimizer = tf.optimizers.Adam()\n",
    "        else:\n",
    "            self.optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        hist = []\n",
    "        const = tf.constant(0.5, dtype=\"float32\")\n",
    "        for _ in range(self.epochs):\n",
    "            with tf.GradientTape() as g:\n",
    "                a1 = tf.matmul(x_train[:, 0], self.W_in)\n",
    "                a2 = tf.matmul(x_train[:, 1], self.W_in)\n",
    "                a3 = tf.add(a1, a2)\n",
    "                \n",
    "                hidden_layer = tf.multiply(const, a3)\n",
    "                \n",
    "                output_layer = tf.nn.softmax(tf.matmul(hidden_layer, self.W_out))\n",
    "                cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(output_layer), axis=[1]))\n",
    "                \n",
    "                grads = g.gradient(cross_entropy_loss, [self.W_in, self.W_out])\n",
    "                self.optimizer.apply_gradients(zip(grads,[self.W_in, self.W_out]))\n",
    "                if(_ % 10==0):\n",
    "                    hist.append(cross_entropy_loss)\n",
    "                if(_ % 1000 == 0):\n",
    "                    print(cross_entropy_loss)\n",
    "                    \n",
    "        self._plot_loss(hist)            \n",
    "        \n",
    "    def _plot_loss(self, hist):\n",
    "        plt.plot(hist)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "w2v = SimpleCBOW(vocab_size=vocab_size, optimizer='sgd', epochs=10000)\n",
    "w2v.train(contexts, target)\n",
    "#def plot_distributed(vector):\n",
    " #       plt.scatter(vector.T[0],vector.T[1])\n",
    "   #     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributed(vector, id_to_word):\n",
    "    for i in range(vector.shape[0]):\n",
    "        plt.plot(vector[i][0], vector[i][1], marker=\"o\", label=id_to_word[i])\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "contexts_tf = tf.convert_to_tensor(contexts)\n",
    "h1 = tf.matmul(contexts_tf, w2v.W_in) \n",
    "h2 = tf.matmul(h1, w2v.W_out)\n",
    "print(w2v.W_in.numpy())\n",
    "print(id_to_word)\n",
    "plot_distributed(w2v.W_in.numpy(), id_to_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
